services:
  # MinIO Object Storage
  # minio:
  #   image: minio/minio:latest
  #   container_name: minio
  #   ports:
  #     - "9000:9000"
  #     - "9001:9001"
  #   environment:
  #     - MINIO_ROOT_USER=minioadmin
  #     - MINIO_ROOT_PASSWORD=minioadmin
  #   command: server /data --console-address ":9001"
  #   volumes:
  #     - minio_data:/data
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 30s
  #   networks:
  #     - default

  # MinIO Client for bucket initialization
  # minio-client:
  #   image: minio/mc:latest
  #   container_name: minio-client
  #   depends_on:
  #     minio:
  #       condition: service_healthy
  #   entrypoint:
  #     - /bin/sh
  #     - -c
  #     - |
  #       until mc alias set local http://minio:9000 minioadmin minioadmin; do
  #         echo 'MinIO not ready yet, waiting...'
  #         sleep 2
  #       done
  #       mc mb local/loki-data --ignore-existing
  #       mc mb local/mimir-data --ignore-existing
  #       mc mb local/tempo-data --ignore-existing
  #       echo 'MinIO buckets initialized successfully'
  #       mc ls local/
  #   networks:
  #     - default

  # Grafana Tempo (Distributed Tracing)
  # tempo:
  #   image: grafana/tempo:latest
  #   container_name: tempo
  #   ports:
  #     - "3200:3200"    # HTTP
  #     - "4317:4317"    # OTLP gRPC
  #     - "4318:4318"    # OTLP HTTP
  #     - "14250:14250"  # Jaeger gRPC
  #     - "14268:14268"  # Jaeger HTTP
  #     - "6831:6831"    # Jaeger UDP
  #     - "6832:6832"    # Jaeger UDP
  #     - "9411:9411"    # Zipkin
  #   command:
  #     - -config.file=/etc/tempo/tempo-config.yml
  #   volumes:
  #     - ./config/tempo/tempo-config.yml:/etc/tempo/tempo-config.yml:ro
  #     - tempo_data:/var/tempo
  #   depends_on:
  #     minio:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:3200/ready"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 90s
  #   networks:
  #     - default

  # Grafana Mimir (Metrics Backend)
  # mimir:
  #   image: grafana/mimir:latest
  #   container_name: mimir
  #   ports:
  #     - "9009:9009"    # HTTP API
  #     - "9095:9095"    # gRPC
  #   command:
  #     - -config.file=/etc/mimir/mimir-config.yml
  #     - -server.http-listen-port=9009
  #   volumes:
  #     - ./config/mimir/mimir-config.yml:/etc/mimir/mimir-config.yml:ro
  #     - mimir_data:/mimir
  #   depends_on:
  #     minio:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:9009/ready"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 90s
  #   networks:
  #     - default

  # Grafana Loki (Log Aggregation)
  # loki:
  #   image: grafana/loki:latest
  #   container_name: loki
  #   ports:
  #     - "3100:3100"
  #   command:
  #     - -config.file=/etc/loki/loki-config.yml
  #   volumes:
  #     - ./config/loki/loki-config.yml:/etc/loki/loki-config.yml:ro
  #     - loki_data:/loki
  #   depends_on:
  #     minio:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:3100/ready"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 90s
  #   networks:
  #     - default

  # Grafana Alloy (Unified Collector)
  # alloy:
  #   image: grafana/alloy:v1.11.2
  #   container_name: alloy
  #   ports:
  #     - "12345:12345"  # Prometheus metrics
  #     - "4327:4317"    # OTLP gRPC (mapped to avoid conflict with Tempo)
  #     - "4328:4318"    # OTLP HTTP (mapped to avoid conflict with Tempo)
  #   command:
  #     - run
  #     - /etc/alloy/config.alloy
  #     - --server.http.listen-addr=0.0.0.0:12345
  #     - --storage.path=/alloy
  #   volumes:
  #     - ./config/alloy/alloy-config.alloy:/etc/alloy/config.alloy:ro
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #     - alloy_data:/alloy
  #   depends_on:
  #     - loki
  #     - tempo
  #     - mimir
  #   # healthcheck:
  #   #   test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:12345/-/healthy || exit 1"]
  #   #   interval: 10s
  #   #   timeout: 5s
  #   #   retries: 5
  #   #   start_period: 30s
  #   networks:
  #     - default

  # Consul server for service discovery
  consul:
    image: hashicorp/consul:1.17
    container_name: consul
    ports:
      - "8500:8500"
    environment:
      - CONSUL_BIND_INTERFACE=eth0
    command: >
      consul agent -dev 
      -client=0.0.0.0 
      -ui 
      -log-level=info 
      -data-dir=/consul/data
    healthcheck:
      test: ["CMD", "consul", "members"]
      interval: 10s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - default
    # volumes:
    #   - ./data/consul-data:/consul/data

  # Config Server
  config-server:
    build:
      context: ./config-server
      dockerfile: Dockerfile
    image: hzeroxium/config-server:latest
    container_name: config-server
    ports:
      - "8888:8888"
    environment:
      # - CONFIG_GIT_URI=file:///app/ztf-spring-cloud-config-server
      - CONFIG_GIT_URI=https://github.com/HZeroxium/ztf-spring-cloud-config-server.git
      - CONFIG_GIT_LABEL=master
      - KAFKA_BROKERS=10.40.30.233:9092
      - CONSUL_HOST=consul
      - CONSUL_PORT=8500
    depends_on:
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Sample Service
  sample-service:
    build:
      context: ./sample-service
      dockerfile: Dockerfile
    image: hzeroxium/sample-service:latest
    container_name: sample-service
    ports:
      - "8080:8080"
    environment:
      - CONFIG_SERVER_URL=http://config-server:8888
      - KAFKA_BROKERS=10.40.30.233:9092
      - SPRING_PROFILES_ACTIVE=dev
      - ZCM_SDK_SERVICE_NAME=sample-service
      - ZCM_SDK_CONFIG_SERVER_URL=http://config-server:8888
      - ZCM_SDK_DISCOVERY_CONSUL_HOST=consul
      - ZCM_SDK_DISCOVERY_CONSUL_HEARTBEAT_ENABLED=true
      - ZCM_SDK_DISCOVERY_CONSUL_REGISTER=true
      - ZCM_SDK_PING_ENABLED=true
      - ZCM_SDK_CONTROL_URL=http://config-control-service:8080
    depends_on:
      config-server:
        condition: service_healthy
      consul:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    stop_grace_period: 30s

  # Scale Sample Service to 3 instances
  # sample-service-2:
  #   build:
  #     context: ./sample-service
  #     dockerfile: Dockerfile
  #   image: hzeroxium/sample-service:latest
  #   container_name: sample-service-2
  #   ports:
  #     - "8082:8080"
  #   environment:
  #     - CONFIG_SERVER_URL=http://config-server:8888
  #     - KAFKA_BROKERS=10.40.30.233:9092
  #     - SPRING_PROFILES_ACTIVE=dev
  #     - ZCM_SDK_SERVICE_NAME=sample-service
  #     - ZCM_SDK_CONFIG_SERVER_URL=http://config-server:8888
  #     - ZCM_SDK_DISCOVERY_CONSUL_HOST=consul
  #     - ZCM_SDK_DISCOVERY_CONSUL_HEARTBEAT_ENABLED=true
  #     - ZCM_SDK_DISCOVERY_CONSUL_REGISTER=true
  #     - ZCM_SDK_PING_ENABLED=true
  #     - ZCM_SDK_CONTROL_URL=http://config-control-service:8080
  #   depends_on:
  #     config-server:
  #       condition: service_healthy
  #     consul:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #   stop_grace_period: 30s


  # Prometheus (Metrics Collection & Scraping)
  # prometheus:
  #   image: prom/prometheus:v3.5.0
  #   container_name: prometheus
  #   ports:
  #     - "9092:9090"  # Prometheus UI and API
  #   volumes:
  #     - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
  #     - prometheus_data:/prometheus
  #   command:
  #     - "--config.file=/etc/prometheus/prometheus.yml"
  #     - "--storage.tsdb.path=/prometheus"
  #     - "--web.console.libraries=/etc/prometheus/console_libraries"
  #     - "--web.console.templates=/etc/prometheus/consoles"
  #     - "--storage.tsdb.retention.time=200h"
  #     - "--web.enable-lifecycle"
  #     - "--web.enable-admin-api"
  #     - "--enable-feature=exemplar-storage"
  #   depends_on:
  #     - mimir
  #   healthcheck:
  #     test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 30s
  #   networks:
  #     - default

  # Grafana (Visualization & Dashboards)
  # grafana:
  #   image: grafana/grafana:main-ubuntu
  #   container_name: grafana
  #   ports:
  #     - "3000:3000"
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
  #     - GF_USERS_ALLOW_SIGN_UP=false
  #     - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
  #     - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor,correlations
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #     - ./config/grafana/grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
  #   depends_on:
  #     - prometheus
  #     - mimir
  #     - loki
  #     - tempo
  #   healthcheck:
  #     test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5
  #     start_period: 30s
  #   networks:
  #     - default

  # Config Control Service
  config-control-service:
    build:
      context: ./config-control-service
      dockerfile: Dockerfile
    image: hzeroxium/config-control-service:latest
    container_name: config-control-service
    ports:
      - "8081:8080"    # HTTP
      - "9090:9090"    # Thrift
      - "9091:9091"    # gRPC
    environment:
      - SPRING_PROFILES_ACTIVE=dev
      - CONFIG_SERVER_URL=http://config-server:8888
      - CONSUL_URL=http://consul:8500
      - MONGODB_URI=mongodb://10.40.30.233:27017/config_control
      - REDIS_URL=redis://10.40.30.233:6379
      - KAFKA_BROKERS=10.40.30.233:9092
      - THRIFT_PORT=9090
      - GRPC_PORT=9091
      - LOGGING_LEVEL_ROOT=WARN
    depends_on:
      config-server:
        condition: service_healthy
      consul:
        condition: service_healthy
      # alloy:
      #   condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # # etcd for KV Store
  # etcd:
  #   image: quay.io/coreos/etcd:v3.5.12
  #   container_name: etcd
  #   ports:
  #     - "2379:2379"
  #     - "2380:2380"
  #   environment:
  #     - ETCD_NAME=etcd0
  #     - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
  #     - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
  #     - ETCD_INITIAL_ADVERTISE_PEER_URLS=http://etcd:2380
  #     - ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
  #     - ETCD_INITIAL_CLUSTER=etcd0=http://etcd:2380
  #     - ETCD_INITIAL_CLUSTER_TOKEN=etcd-cluster
  #     - ETCD_INITIAL_CLUSTER_STATE=new
  #   healthcheck:
  #     test: ["CMD", "etcdctl", "endpoint", "health"]
  #     interval: 10s
  #     timeout: 3s
  #     retries: 5


volumes:
  minio_data:
  tempo_data:
  mimir_data:
  loki_data:
  alloy_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    name: config-demo-network
    driver: bridge


